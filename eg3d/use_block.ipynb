{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705\n",
      "makeup/012ce0437aec38b7b42918f3e4971c13.png\n",
      "makeup/012ce0437aec38b7b42918f3e4971c13.png\n",
      "seg_files: 012ce0437aec38b7b42918f3e4971c13.png\n",
      "seg: makeup/012ce0437aec38b7b42918f3e4971c13.png\n",
      "2705\n",
      "2697\n",
      "False\n",
      "2697\n"
     ]
    }
   ],
   "source": [
    "# remake datasets\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "from inference import get_latents\n",
    "from utils.model_utils import setup_model\n",
    "device = torch.device('cuda')\n",
    "import os\n",
    "dir_name = 'makeup'\n",
    "latent_path = os.path.join('/media/pc/hengda1t/hengda/datasets/latents/',dir_name)\n",
    "latent_makeup_dir = os.listdir(latent_path)\n",
    "print(len(latent_makeup_dir))\n",
    "# latents\n",
    "latent_files = []\n",
    "for x in latent_makeup_dir:\n",
    "    name = os.path.join(dir_name,x)\n",
    "    latent_files.append(name.replace('.pt','.png'))\n",
    "print(latent_files[0])\n",
    "set_latents = set(latent_files)\n",
    "\n",
    "# lms\n",
    "lms_path = os.path.join('/media/pc/hengda1t/hengda/datasets/MT-Dataset/lms',dir_name)\n",
    "lms_files = os.listdir(lms_path)\n",
    "lms_names = []\n",
    "for x in lms_files:\n",
    "    name = os.path.join(dir_name,x)\n",
    "    lms_names.append(name.replace('.npy','.png'))\n",
    "print(lms_names[0])\n",
    "set_lms = set(lms_names)\n",
    "\n",
    "#seg\n",
    "seg_path = os.path.join('/media/pc/hengda1t/hengda/datasets/MT-Dataset/segs',dir_name)\n",
    "seg_files = os.listdir(seg_path)\n",
    "print('seg_files:',seg_files[0])\n",
    "seg_names = []\n",
    "for x in seg_files:\n",
    "    name = os.path.join(dir_name,x)\n",
    "    seg_names.append(name)\n",
    "print('seg:',seg_names[0])\n",
    "set_seg = set(seg_names)\n",
    "\n",
    "\n",
    "a = set_seg & set_lms & set_latents\n",
    "print(len(set_latents))\n",
    "print(len(set_lms))\n",
    "print(set_seg<=set_lms)\n",
    "print(len(a))\n",
    "with open(dir_name+'.txt','w') as f :\n",
    "    for x in a:\n",
    "        f.write(x +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan])\n",
      "tensor([nan])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([float('nan')])\n",
    "print(a)\n",
    "if a is not None:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n"
     ]
    }
   ],
   "source": [
    "print(5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#引入EG3D\n",
    "\n",
    "from training.triplane import TriPlaneGenerator\n",
    "import dnnlib\n",
    "import legacy\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "## end main\n",
    "import sys\n",
    "sys.path.append('/media/pc/hengda1t/hengda/e4e_eg3d/models/')\n",
    "latent = torch.randn(4,14,512)\n",
    "seq = torch.randn(4,197,768)\n",
    "fusion = SimpleSelfCrossTransformer(num_layers=6, style_dim=512, heads=8, num_styles=14, inject_layers=None)\n",
    "\n",
    "logit = fusion(reference=seq, styles=latent)\n",
    "print(logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pseudo ground truth\n",
    "from config.configs import get_config\n",
    "from config.pgtmaker_loss import AnnealingComposePGT, ComposePGT,MakeupLoss\n",
    "from config.preprocess import PreProcess\n",
    "import os \n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "pgt_annealing = True\n",
    "configs = get_config()\n",
    "img_size = configs.DATA.IMG_SIZE\n",
    "\n",
    "device = 'cuda:2'\n",
    "margins = {'eye':configs.PGT.EYE_MARGIN,\n",
    "           'lip':configs.PGT.LIP_MARGIN}\n",
    "pgt_maker =AnnealingComposePGT(margins, \n",
    "                configs.PGT.SKIN_ALPHA_MILESTONES, configs.PGT.SKIN_ALPHA_VALUES,\n",
    "                configs.PGT.EYE_ALPHA_MILESTONES, configs.PGT.EYE_ALPHA_VALUES,\n",
    "                configs.PGT.LIP_ALPHA_MILESTONES, configs.PGT.LIP_ALPHA_VALUES\n",
    "            ) \n",
    "pgt_maker.eval()\n",
    "dest_datasets = './dataset/MTdatasets_test/'\n",
    "with open('./dataset/MTdatasets_test/makeup_test.txt','r') as f :\n",
    "    makeup_names = [name.strip() for name in f.readlines()]\n",
    "\n",
    "with open('./dataset/MTdatasets_test/non-makeup_test.txt','r') as f :\n",
    "    non_makeup_names = [name.strip() for name in f.readlines()]\n",
    "\n",
    "preprocessor = PreProcess(configs,need_parser=False)\n",
    "PATH_DATA_ROOT = './dataset/MTdatasets_test/'\n",
    "\n",
    "def load_from_file(img_name):\n",
    "    image = Image.open(os.path.join(PATH_DATA_ROOT,'images',img_name)).convert('RGB')\n",
    "    mask = preprocessor.load_mask(os.path.join(PATH_DATA_ROOT,'segs',img_name))\n",
    "    base_name = os.path.splitext(img_name)[0]\n",
    "    lms = preprocessor.load_lms(os.path.join(PATH_DATA_ROOT,'lms',f'{base_name}.npy'))\n",
    "    return preprocessor.process(image,mask,lms)\n",
    "# load source and reference\n",
    "sources = []\n",
    "references = []\n",
    "imgs_s , imgs_r = [],[]\n",
    "maskes_s, maskes_r = [],[]\n",
    "diffs_s, diffs_r = [],[]\n",
    "lmses_s,lmses_r = [],[]\n",
    "for idx  in range(0,len(makeup_names)):\n",
    "    name_s = non_makeup_names[idx]\n",
    "    name_r = makeup_names[idx]\n",
    "    print('makeup_names[%d]:' %idx,makeup_names[idx])\n",
    "    print('non_makeup_names[%d]:' %idx ,non_makeup_names[idx])\n",
    "    source = load_from_file(name_s)\n",
    "    reference = load_from_file(name_r)\n",
    "    sources.append(source)\n",
    "    references.append(reference)\n",
    "    imgs_s.append(sources[idx][0]),imgs_r.append(references[idx][0])\n",
    "    maskes_s.append(sources[idx][1]),maskes_r.append(references[idx][1])\n",
    "    diffs_s.append(sources[idx][2]),diffs_r.append(references[idx][2])\n",
    "    lmses_s.append(sources[idx][3]),lmses_r.append(references[idx][3])\n",
    "    \n",
    "    pgt_A = pgt_maker(imgs_s[idx].unsqueeze(0).to(device),imgs_r[idx].unsqueeze(0).to(device),\n",
    "                      maskes_s[idx].unsqueeze(0).to(device),maskes_r[idx].unsqueeze(0).to(device),\n",
    "                      lmses_s[idx].unsqueeze(0).to(device),lmses_r[idx].unsqueeze(0).to(device))\n",
    "    # print(pgt_A)\n",
    "    image = pgt_A.cpu().clone()\n",
    "    print('name_%d_torch:' %idx ,+' ',image)\n",
    "    image = transforms.ToPILImage()(image.squeeze(0))\n",
    "    print('name_%d_image:' %idx ,+' ',image)\n",
    "    #image.save('./pgt_test/test_%d.png' %idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the net parameters\n",
    "import torch\n",
    "import clip\n",
    "print(clip)\n",
    "from PIL import Image\n",
    "device =\"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "net_farl, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "net_farl = net_farl.to(device)\n",
    "farl_state=torch.load(\"../pretrained_models//FaRL-Base-Patch16-LAIONFace20M-ep16.pth\") # you can download from https://github.com/FacePerceiver/FaRL#pre-trained-backbones\n",
    "\n",
    "net_farl.load_state_dict(farl_state[\"state_dict\"],strict=False)\n",
    "image = preprocess(Image.open(\"../save_imgs_test/inversions/xfsy_0002.jpg\")).unsqueeze(0).to(device)\n",
    "print(net_farl.parameters())\n",
    "for name, parms in net_farl.named_parameters():\n",
    "    print('-->name:', name, '-->grad_requirs:',parms.requires_grad, \\\n",
    "    ' -->grad_value:',parms.grad)\n",
    "a = net_farl.encode_image(image)\n",
    "print('a:',a.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('wyq_eg3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd8aa1e353085ab8c9d6acf1eecd95a62bbd8c85e2ba57ed7c25982a5575dde6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
