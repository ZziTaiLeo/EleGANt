{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use optimazation to find the best quality\n",
    "%matplotlib inline\n",
    "from torchvision.transforms import functional\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from mt_training.config import get_config\n",
    "from mt_training.dataset import MakeupDataset\n",
    "from mt_training.solver import Solver\n",
    "import torchvision.transforms as transforms\n",
    "from mt_training.utils import create_logger, print_args\n",
    "from models.loss import GANLoss, MakeupLoss, ComposePGT, AnnealingComposePGT\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import dnnlib\n",
    "import legacy\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from mt_training.preprocess import PreProcess\n",
    "from models.model import get_discriminator, get_generator, vgg16\n",
    "import matplotlib.pyplot as plt\n",
    "from models.modules.pseudo_gt import expand_area\n",
    "from eg3d.inference import get_latents\n",
    "from utils.model_utils import setup_model\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from utils import common, train_utils\n",
    "dataset_json = '/media/pc/LabServers/hengda/datasets/MT-Dataset/images/all_mt_dataset.json'\n",
    "config = get_config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# init psp net\n",
    "\n",
    "lip_class = [12,13]\n",
    "eyebrow_class =[2,3]\n",
    "face_class = [1,10]\n",
    "eye_class = [4,5]\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "def get_landmark(filepath, predictor):\n",
    "    \"\"\"get landmark with dlib\n",
    "    :return: np.array shape=(68, 2)\n",
    "    \"\"\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    img = dlib.load_rgb_image(filepath)\n",
    "    dets = detector(img, 1)\n",
    "    for k, d in enumerate(dets):\n",
    "        shape = predictor(img, d)\n",
    "\n",
    "    t = list(shape.parts())\n",
    "    a = []\n",
    "    for tt in t:\n",
    "        a.append([tt.x, tt.y])\n",
    "    lm = np.array(a)\n",
    "    lm = rotate(lm,90).transpose(1,0)\n",
    "    return torch.IntTensor(lm)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config.DATA.IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "model_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/pretrained_models/shape_predictor_68_face_landmarks.dat'\n",
    "predictor = dlib.shape_predictor(\n",
    "model_path)\n",
    "def load_eg3d(network_pkl):\n",
    "    with dnnlib.util.open_url(network_pkl) as f:\n",
    "        G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "        G.eval()\n",
    "    return G\n",
    "\n",
    "img_size = 512\n",
    "xs, ys = np.meshgrid(\n",
    "    np.linspace(\n",
    "        0, img_size - 1,\n",
    "        img_size\n",
    "    ),\n",
    "    np.linspace(\n",
    "        0, img_size - 1,\n",
    "        img_size\n",
    "    )\n",
    ")\n",
    "xs = xs[None].repeat(config.PREPROCESS.LANDMARK_POINTS, axis=0)\n",
    "ys = ys[None].repeat(config.PREPROCESS.LANDMARK_POINTS, axis=0)\n",
    "fix = np.concatenate([ys, xs], axis=0) \n",
    "fix = torch.Tensor(fix) #(136, h, w)\n",
    "def diff_process(lms: torch.Tensor, normalize=False):\n",
    "    '''\n",
    "    lms:(68, 2)\n",
    "    '''\n",
    "    lms = lms.transpose(1, 0).reshape(-1, 1, 1) # (136, 1, 1)\n",
    "    diff = fix - lms # (136, h, w)\n",
    "\n",
    "    if normalize:\n",
    "        norm = torch.norm(diff, dim=0, keepdim=True).repeat(diff.shape[0], 1, 1)\n",
    "        norm = torch.where(norm == 0, torch.tensor(1e10), norm)\n",
    "        diff /= norm\n",
    "    return diff\n",
    "\n",
    "\n",
    "# pgt \n",
    "margins = {'eye':config.PGT.EYE_MARGIN,\n",
    "                'lip':config.PGT.LIP_MARGIN}\n",
    "#pseudo ground truth\n",
    "def init_pgt():\n",
    "    pgt_maker = AnnealingComposePGT(margins, \n",
    "                    config.PGT.SKIN_ALPHA_MILESTONES, config.PGT.SKIN_ALPHA_VALUES,\n",
    "                    config.PGT.EYE_ALPHA_MILESTONES, config.PGT.EYE_ALPHA_VALUES,\n",
    "                    config.PGT.LIP_ALPHA_MILESTONES, config.PGT.LIP_ALPHA_VALUES\n",
    "                )\n",
    "    pgt_maker.eval()\n",
    "    return pgt_maker\n",
    "\n",
    "def load_from_file(img_name):\n",
    "    path_img = os.path.join(PATH_DATA_ROOT,'images',img_name) \n",
    "    image = Image.open(path_img).convert('RGB')\n",
    "    mask = load_mask(os.path.join(PATH_DATA_ROOT,'segs',img_name))\n",
    "\n",
    "    path_lms =os.path.join(PATH_DATA_ROOT,'lms',img_name.replace('.png','.npy')) \n",
    "    lms = load_lms(path_lms)\n",
    "    # lms = get_landmark(path_img, predictor)\n",
    "\n",
    "    # plt.scatter(lms[:,0],lms[:,1])\n",
    "    # plt.show()\n",
    "    mask = mask_process(mask)\n",
    "    #TODO \n",
    "    diff = diff_process(lms)\n",
    "    img = transform(image)\n",
    "    return [img, mask, diff, lms]\n",
    "\n",
    "def load_lms(path):\n",
    "    lms = np.load(path)\n",
    "    return torch.IntTensor(lms)\n",
    "\n",
    "\n",
    "def load_mask(path):\n",
    "    mask = np.array(Image.open(path).convert('L'))\n",
    "    mask = torch.FloatTensor(mask).unsqueeze(0)\n",
    "    mask = functional.resize(mask, 512, transforms.InterpolationMode.NEAREST)\n",
    "    return mask\n",
    "def mask_process( mask: torch.tensor):\n",
    "    '''\n",
    "    mask: (1, h, w)\n",
    "    '''        \n",
    "    mask_lip = (mask == lip_class[0]).float() + (mask == lip_class[1]).float()\n",
    "    mask_face = (mask == face_class[0]).float() + (mask == face_class[1]).float()\n",
    "\n",
    "    #mask_eyebrow_left = (mask == eyebrow_class[0]).float()\n",
    "    #mask_eyebrow_right = (mask == eyebrow_class[1]).float()\n",
    "    mask_face += (mask == eyebrow_class[0]).float()\n",
    "    mask_face += (mask == eyebrow_class[1]).float()\n",
    "\n",
    "    mask_eye_left = (mask == eye_class[0]).float()\n",
    "    mask_eye_right = (mask == eye_class[1]).float()\n",
    "\n",
    "    #mask_list = [mask_lip, mask_face, mask_eyebrow_left, mask_eyebrow_right, mask_eye_left, mask_eye_right]\n",
    "    mask_list = [mask_lip, mask_face, mask_eye_left, mask_eye_right]\n",
    "    mask_aug = torch.cat(mask_list, 0) # (c, h, w)\n",
    "    return mask_aug      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# image_r (b,c,h,w)\n",
    "def forward(self, s_latent, c):\n",
    "    s_latent = s_latent.to(self.device).float()\n",
    "    img = self.G.synthesis(s_latent, c, noise_mode='const')['image']\n",
    "    # print('img_synthesis.shape:', img.shape)\n",
    "    return img\n",
    "\n",
    "\n",
    "# save img\n",
    "\n",
    "def de_norm( x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "def tensor2im(var):\n",
    "    # var shape: (3, H, W)\n",
    "    print(var.shape)\n",
    "    var = var[0].cpu().detach().transpose(0, 2).transpose(0, 1).numpy()\n",
    "    var = ((var + 1) / 2)\n",
    "    var[var < 0] = 0\n",
    "    var[var > 1] = 1\n",
    "    var = var * 255\n",
    "    return Image.fromarray(var.astype('uint8')) \n",
    "\n",
    "def vis_train(img_train_batch, name_s,name_r,step=None):\n",
    "    # saving training results\n",
    "    display_count=1\n",
    "    print(img_train_batch)\n",
    "    fig = plt.figure(figsize=(5*len(img_train_batch),8*display_count ))\n",
    "    gs = fig.add_gridspec(display_count,len(img_train_batch))\n",
    "    for i in range(display_count):\n",
    "        fig.add_subplot(gs[i,0])\n",
    "        vis_face(img_train_batch,fig,gs,i,name_s,name_r)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(vis_folder,f'{step}_{name_s}'))# add name\n",
    "    plt.close(fig)\n",
    "# 制作一个输出的图\n",
    "def vis_face(img_batch, fig, gs,i,name_s,name_r):\n",
    "    print('img_batch:',img_batch)\n",
    "    plt.imshow(img_batch[0])\n",
    "    plt.title(f'Input:{name_s}')\n",
    "    \n",
    "    fig.add_subplot(gs[i,1])\n",
    "    plt.imshow(img_batch[1])\n",
    "    plt.title(f'Inversion')\n",
    "\n",
    "    fig.add_subplot(gs[i,2])\n",
    "    plt.imshow(img_batch[2])\n",
    "    plt.title(f'Reference:{name_r}')\n",
    "    \n",
    "    fig.add_subplot(gs[i,3])\n",
    "    plt.imshow(img_batch[3])\n",
    "    plt.title('Optimization Result')\n",
    "    \n",
    "    fig.add_subplot(gs[i,4])\n",
    "    plt.imshow(img_batch[4])\n",
    "    plt.title('PGT')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# #epoch 89\n",
    "# source_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_89/xfsy_0232.png'\n",
    "# source_img_latent_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_89/xfsy_0232.npy'\n",
    "# reference_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_89/vFG152.png'\n",
    "# #epoch 94\n",
    "# source_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_94/xfsy_0221.png'\n",
    "# source_img_latent_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_94/xfsy_0221.npy'\n",
    "# reference_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_94/vFG136.png'\n",
    "# #epoch 100\n",
    "# source_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_100/xfsy_0394.png'\n",
    "# source_img_latent_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_100/xfsy_0394.npy'\n",
    "# reference_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_100/vFG545.png'\n",
    "# #epoch 106\n",
    "# source_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_106/xfsy_0352.png'\n",
    "# source_img_latent_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_106/xfsy_0352.npy'\n",
    "# reference_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_106/vFG410.png'\n",
    "# #epoch 112\n",
    "# source_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_112/xfsy_0446.png'\n",
    "# source_img_latent_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_112/xfsy_0446.npy'\n",
    "# reference_img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/epoch_112/vHX699.png'\n",
    "\n",
    "\n",
    "file_path = 'makeup/vHX544.png'\n",
    "source_img_latent_path ='/media/pc/LabServers/hengda/datasets/latents/makeup/vHX544.pt' \n",
    "reference_img_path = 'non-makeup/vSYYZ267.png'\n",
    "\n",
    "num_steps =  1000\n",
    "basename_s = os.path.basename(file_path)\n",
    "basename_r = os.path.basename(reference_img_path)\n",
    "# load eg3d decoder\n",
    "pgt_maker = init_pgt()\n",
    "pgt_maker = pgt_maker.to(device)\n",
    "network_pkl = '../pretrained_models/ffhq512-128.pkl'\n",
    "G = load_eg3d(network_pkl)\n",
    "G = G.to(device)\n",
    "# save your img \n",
    "save_folder='test_for_optimization'\n",
    "vis_folder = os.path.join(save_folder, 'visualization',basename_s[:-4]+'_crop_tran_lm')\n",
    "os.makedirs(vis_folder,exist_ok=True)\n",
    "# input params\n",
    "# load latents\n",
    "\n",
    "# latent_s = torch.from_numpy(np.load(source_img_latent_path))\n",
    "latent_s = torch.load(source_img_latent_path)\n",
    "latent_s = latent_s.to(device)\n",
    "latent_s.requires_grad =True\n",
    "print(latent_s.requires_grad)\n",
    "\n",
    "#load camera\n",
    "with open(dataset_json, 'r') as f:\n",
    "    camera_dic = dict(json.load(f)['labels'])\n",
    "\n",
    "c_s = camera_dic[basename_s]\n",
    "c_s = torch.tensor(c_s).unsqueeze(0)\n",
    "c_s = c_s.to(device=device)\n",
    "# init optimizer\n",
    "# g_lr = config.TRAINING.G_LR\n",
    "g_lr = 0.1\n",
    "lr_decay_factor = config.TRAINING.LR_DECAY_FACTOR\n",
    "optimizer = torch.optim.Adam([latent_s] , betas=(0.9, 0.999),\n",
    "                                 lr=g_lr)\n",
    "g_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "T_max=num_steps, eta_min=g_lr * lr_decay_factor)\n",
    "# Loss param\n",
    "\n",
    "lambda_idt      = config.LOSS.LAMBDA_IDT\n",
    "lambda_A        = config.LOSS.LAMBDA_A\n",
    "lambda_lip  = config.LOSS.LAMBDA_MAKEUP_LIP\n",
    "lambda_skin = config.LOSS.LAMBDA_MAKEUP_SKIN\n",
    "lambda_eye  = config.LOSS.LAMBDA_MAKEUP_EYE\n",
    "lambda_vgg      = config.LOSS.LAMBDA_VGG\n",
    "\n",
    "#load discriminator\n",
    "D_A = get_discriminator(config)\n",
    "D_A = D_A.to(device=device)\n",
    "\n",
    "# pgt_input maker\n",
    "preprocessor = PreProcess(config,need_parser=False)\n",
    "PATH_DATA_ROOT = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/'\n",
    "# source = load_from_file(source_img_path)\n",
    "# reference = load_from_file(reference_img_path)\n",
    "# image_s,image_r = source[0].unsqueeze(0).to(device), reference[0].unsqueeze(0).to(device)\n",
    "# mask_s_full,mask_r_full = source[1].unsqueeze(0).to(device), reference[1].unsqueeze(0).to(device) \n",
    "# diff_s,diff_r = source[2].unsqueeze(0).to(device), reference[2].unsqueeze(0).to(device)\n",
    "# lms_s, lms_r= source[3].unsqueeze(0).to(device), reference[3].unsqueeze(0).to(device)\n",
    "\n",
    "source = load_from_file(file_path)\n",
    "reference = load_from_file(reference_img_path)\n",
    "image_s,image_r = source[0].unsqueeze(0).to(device), reference[0].unsqueeze(0).to(device)\n",
    "mask_s_full,mask_r_full = source[1].unsqueeze(0).to(device), reference[1].unsqueeze(0).to(device) \n",
    "diff_s,diff_r = source[2].unsqueeze(0).to(device), reference[2].unsqueeze(0).to(device)\n",
    "lms_s, lms_r= source[3].unsqueeze(0).to(device), reference[3].unsqueeze(0).to(device)\n",
    "\n",
    "pgt_A = pgt_maker(image_s,image_r,\n",
    "                  mask_s_full,mask_r_full,\n",
    "                  lms_s, lms_r)\n",
    "\n",
    "\n",
    "#optimaze the code \n",
    "\n",
    "# design loss\n",
    "criterionL1 = torch.nn.L1Loss()\n",
    "criterionL2 = torch.nn.MSELoss()\n",
    "criterionGAN = GANLoss(gan_mode='lsgan')\n",
    "criterionPGT = MakeupLoss()\n",
    "vgg = vgg16(pretrained=True)\n",
    "vgg = vgg.to(device)\n",
    "\n",
    "for step in tqdm(range(num_steps)):\n",
    "    # go a generator\n",
    "    fake_A= G.synthesis(latent_s, c_s, noise_mode='const')['image']\n",
    "    if step ==0:\n",
    "        inversion_origin = fake_A\n",
    "\n",
    "    # ================== Train G ================== #\n",
    "    # G should be identity if ref_B or org_A is fed\n",
    "\n",
    "    loss_idt_A = criterionL1(fake_A, image_s) * lambda_A * lambda_idt\n",
    "    loss_idt = (loss_idt_A )\n",
    "\n",
    "    # GAN loss D_A(G_A(A))\n",
    "    pred_fake = D_A(fake_A)\n",
    "    g_A_loss_adv = criterionGAN(pred_fake, True)\n",
    "\n",
    "    # Makeup loss\n",
    "    g_A_loss_pgt = 0; \n",
    "\n",
    "    g_A_lip_loss_pgt = criterionPGT(fake_A, pgt_A, mask_s_full[:,0:1]) * lambda_lip\n",
    "    g_A_loss_pgt += g_A_lip_loss_pgt\n",
    "\n",
    "    mask_s_eye = expand_area(mask_s_full[:,2:4].sum(dim=1, keepdim=True), margins['eye'])\n",
    "    mask_r_eye = expand_area(mask_r_full[:,2:4].sum(dim=1, keepdim=True), margins['eye'])\n",
    "    mask_s_eye = mask_s_eye * mask_s_full[:,1:2]\n",
    "    mask_r_eye = mask_r_eye * mask_r_full[:,1:2]\n",
    "    g_A_eye_loss_pgt = criterionPGT(fake_A, pgt_A, mask_s_eye) * lambda_eye\n",
    "    g_A_loss_pgt += g_A_eye_loss_pgt\n",
    "\n",
    "    mask_s_skin = mask_s_full[:,1:2] * (1 - mask_s_eye)\n",
    "    mask_r_skin = mask_r_full[:,1:2] * (1 - mask_r_eye)\n",
    "    g_A_skin_loss_pgt = criterionPGT(fake_A, pgt_A, mask_s_skin) * lambda_skin\n",
    "    g_A_loss_pgt += g_A_skin_loss_pgt\n",
    "\n",
    "    # vgg loss\n",
    "    vgg_s = vgg(image_s).detach()\n",
    "    vgg_fake_A = vgg(fake_A)\n",
    "    g_loss_A_vgg = criterionL2(vgg_fake_A, vgg_s) * lambda_A * lambda_vgg\n",
    "    vgg_r = vgg(image_r).detach()\n",
    "    loss_rec = g_loss_A_vgg\n",
    "\n",
    "    # Combined loss\n",
    "    g_loss = g_A_loss_adv + loss_rec + loss_idt + g_A_loss_pgt\n",
    "    optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    optimizer.step()\n",
    "    g_scheduler.step()\n",
    "    # for param_group in optimizer.param_groups:\n",
    "    #     print('学习率：',param_group['lr'])\n",
    "    # print(\"Step: %d, Loss_G: %0.4f, loss_adv: %0.4f, loss_vgg: %0.4f, loss_id: %0.4f, loss_pgt: %0.4f\" % \\\n",
    "    # (step + 1,g_loss, g_A_loss_adv, loss_rec,loss_idt,g_A_loss_pgt ))\n",
    "    if (step+1) %100==0:\n",
    "        vis_train([tensor2im(image_s.detach().cpu()),\n",
    "                tensor2im(inversion_origin.detach().cpu()),\n",
    "                tensor2im(image_r.detach().cpu()),\n",
    "                tensor2im(fake_A.detach().cpu()),\n",
    "                tensor2im(pgt_A.detach().cpu())],\n",
    "                basename_s, basename_r, step=step+1\n",
    "                )\n",
    "print('finish!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import legacy\n",
    "import torch\n",
    "\n",
    "def load_eg3d(network_pkl):\n",
    "    with dnnlib.util.open_url(network_pkl) as f:\n",
    "        G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "        G.eval()\n",
    "    return G\n",
    "network_pkl = '../pretrained_models/ffhq512-128.pkl'\n",
    "G = load_eg3d(network_pkl)\n",
    "G = G.to(device)\n",
    "img_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/results/w_plus/optimizer/00022_w_plus/00022.npy'\n",
    "import json \n",
    "\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyq_eg3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd8aa1e353085ab8c9d6acf1eecd95a62bbd8c85e2ba57ed7c25982a5575dde6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
