{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make small scale datasets\n",
    "path_dest_datasets_root = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop-small/'\n",
    "non_makeup_txt_path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/non-makeup.txt'\n",
    "makeup_txt_path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/makeup.txt'\n",
    "with open(non_makeup_txt_path,'r') as f :\n",
    "    list_non_makeup = [x.strip() for x in f.readlines()]\n",
    "with open(makeup_txt_path,'r') as f :\n",
    "    list_makeup = [x.strip() for x in f.readlines()]\n",
    "set_train_nmk = set()\n",
    "set_train_mk = set()\n",
    "set_test_nmk = set()\n",
    "set_test_mk = set()\n",
    "import random\n",
    "for train_nmk in list_non_makeup:\n",
    "    if random.random() <= 0.7:\n",
    "        set_train_nmk.add(train_nmk)\n",
    "    else:\n",
    "        set_test_nmk.add(train_nmk)\n",
    "for train_mk in list_makeup:\n",
    "    if random.random() <= 0.7:\n",
    "        set_train_mk.add(train_mk)\n",
    "    else:\n",
    "        set_test_mk.add(train_mk)\n",
    "with open('/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/train-non-makeup.txt','a') as f :\n",
    "    for x in set_train_nmk:\n",
    "        f.write(x+'\\n')\n",
    "with open('/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/train-makeup.txt','a') as f :\n",
    "    for x in set_train_mk:\n",
    "        f.write(x+'\\n')\n",
    "with open('/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/test-makeup.txt','a') as f :\n",
    "    for x in set_test_mk:\n",
    "        f.write(x+'\\n')\n",
    "with open('/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/test-non-makeup.txt','a') as f :\n",
    "    for x in set_test_nmk:\n",
    "        f.write(x+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from mt_training.config import get_config\n",
    "from fvcore.common.config import CfgNode\n",
    "config = get_config()\n",
    "a = CfgNode.load_yaml_with_base(config)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make small scale datasets\n",
    "path_dest_datasets_root = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/'\n",
    "path_source_datasets_root = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/'\n",
    "non_makeup_txt_path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/non-makeup.txt'\n",
    "makeup_txt_path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/makeup.txt'\n",
    "with open(non_makeup_txt_path,'r') as f :\n",
    "    list_non_makeup = [x.strip() for x in f.readlines()]\n",
    "with open(makeup_txt_path,'r') as f :\n",
    "    list_makeup = [x.strip() for x in f.readlines()]\n",
    "set_train_nmk = set()\n",
    "set_train_mk = set()\n",
    "set_test_nmk = set()\n",
    "set_test_mk = set()\n",
    "import random\n",
    "for x in list_non_makeup:\n",
    "    if ('mirror' not in x ):\n",
    "        set_train_nmk.add(x)\n",
    "for x in list_makeup:\n",
    "    if ('mirror' not in x ):\n",
    "        set_train_mk.add(x)\n",
    "with open('/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/non-makeup_no_mirror.txt','a') as f :\n",
    "    for x in set_train_nmk:\n",
    "        f.write(x+'\\n')\n",
    "with open('/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/makeup_no_mirror.txt','a') as f :\n",
    "    for x in set_train_mk:\n",
    "        f.write(x+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "img_1_path = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/crop_error_img_1.txt'\n",
    "origin_non_makeup_path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/non-makeup_origin.txt'\n",
    "origin_makeup_path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/makeup_origin.txt'\n",
    "non_makeup = []\n",
    "makeup = []\n",
    "with open(img_1_path,'r') as f:\n",
    "   for line in f.readlines():\n",
    "        if 'non-make' in line:\n",
    "            non_makeup.append(line.strip('\\n'))         \n",
    "        else :\n",
    "            makeup.append(line.strip('\\n'))\n",
    "non_makeup_origin = []\n",
    "makeup_origin = []\n",
    "with open(origin_non_makeup_path,'r') as f :\n",
    "    for line in f.readlines():\n",
    "        non_makeup_origin.append(line.strip('\\n'))\n",
    "with open(origin_makeup_path,'r') as f :\n",
    "    for line in f.readlines():\n",
    "        makeup_origin.append(line.strip('\\n'))\n",
    "set_non_makeup = set(non_makeup) \n",
    "set_makeup = set(makeup)\n",
    "set_non_makeup_origin = set(non_makeup_origin)\n",
    "set_makeup_origin = set(makeup_origin)\n",
    "result_non_makeup = set_non_makeup_origin - set_non_makeup\n",
    "result_makeup = set_makeup_origin - set_makeup\n",
    "print(len(result_non_makeup))\n",
    "result_makeup_path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/makeup.txt'\n",
    "result_non_makeup_path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/non_makeup.txt'\n",
    "with open(result_makeup_path,'a') as f:\n",
    "    for x in result_makeup :\n",
    "        f.write(x+'\\n')\n",
    "with open(result_non_makeup_path,'a') as f:\n",
    "    for x in result_non_makeup :\n",
    "        f.write(x+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from skimage import io\n",
    "import os, sys\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from models.modules import tps\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/lms/makeup/87989.npy'\n",
    "path_2 = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/lms/non-makeup/xfsy_0238.npy'\n",
    "a = np.load(path)\n",
    "b = np.load(path_2)\n",
    "pts_before_t = torch.from_numpy(a).unsqueeze(0).float()\n",
    "pts_after_t = torch.from_numpy(b).unsqueeze(0).float()\n",
    "print(a.min())\n",
    "c = torch.flip(pts_before_t,dims=[1])\n",
    "pts_before_t = pts_before_t /512\n",
    "pts_after_t = pts_after_t /512\n",
    "pts_before_t = torch.flip(pts_before_t,dims=[1])\n",
    "pts_after_t = torch.flip(pts_after_t,dims=[1])\n",
    "im = (torch.from_numpy(io.imread('/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/vHX544.png').astype('float32'))\n",
    "      .permute(2, 0, 1)\n",
    "      .unsqueeze(0))/256\n",
    "warp = tps.WarpTPS()\n",
    "newim = warp(im, pts_before_t[0:1,48:,:], pts_after_t[0:1,48:,:])\n",
    "print(newim.shape)\n",
    "# # for i in range(len(pts_after[0])):\n",
    "# #     newim[0,:,pts_after[0][i][0], pts_after[0][i][1]] = torch.Tensor([1,0,0])\n",
    "\n",
    "# torchvision.utils.save_image(\n",
    "#     torchvision.utils.make_grid(newim), 'result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at landmarks\n",
    "\n",
    "import torch\n",
    "path = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/lms/makeup/vHX544.npy'\n",
    "path_2 = '/media/pc/LabServers/hengda/datasets/MT-Dataset-crop/lms/non-makeup/vSYYZ267.npy'\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "a = np.load(path)\n",
    "b = np.load(path_2)\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "\n",
    "print(a.shape)\n",
    "plt.scatter(a[:,0],a[:,1])\n",
    "for i in range(len(a)):\n",
    "    plt.annotate(i,(a[i,0],a[i,1]))\n",
    "plt.show()\n",
    "plt.scatter(b[:,0],b[:,1])\n",
    "for i in range(len(b)):\n",
    "    plt.annotate(i,(b[i,0],b[i,1]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmarks\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "def get_landmark(filepath, predictor):\n",
    "    \"\"\"get landmark with dlib\n",
    "    :return: np.array shape=(68, 2)\n",
    "    \"\"\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    img = dlib.load_rgb_image(filepath)\n",
    "    dets = detector(img, 1)\n",
    "    for k, d in enumerate(dets):\n",
    "        shape = predictor(img, d)\n",
    "\n",
    "    t = list(shape.parts())\n",
    "    a = []\n",
    "    for tt in t:\n",
    "        a.append([tt.x, tt.y])\n",
    "    lm = np.array(a)\n",
    "    lm = rotate(lm,90).transpose(1,0)\n",
    "    return lm\n",
    "model_path = '../pretrained_models/shape_predictor_68_face_landmarks.dat'\n",
    "predictor = dlib.shape_predictor(\n",
    "    model_path)\n",
    "\n",
    "input_dir_type = 'non-makeup'\n",
    "input_img_path = f'/media/pc/LabServers/hengda/datasets/MT-Dataset/images/{input_dir_type}'\n",
    "files = os.listdir(input_img_path)\n",
    "save_dir_path = f'/media/pc/LabServers/hengda/datasets/MT-Dataset/lms_crop/{input_dir_type}'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "for file in files:\n",
    "    if '.png' in file:\n",
    "        try:\n",
    "            source_img_path = os.path.join(input_img_path,file)\n",
    "            lm = get_landmark(source_img_path, predictor)\n",
    "            np.save(os.path.join(save_dir_path,file.replace('.png','.npy')), lm)\n",
    "        except:\n",
    "            print('error_file is :',file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder result latents  .pt-> .npy\n",
    "import torch \n",
    "import numpy as np \n",
    "path = '/media/pc/LabServers/hengda/e4e_eg3d/save_encoder/inversions/latents/'\n",
    "dest_path = '/media/pc/LabServers/hengda/EG3D-projector/eg3d/projector_out/encoder'\n",
    "files = os.listdir(path)\n",
    "for file in files:\n",
    "    files_path = os.path.join(path,file)\n",
    "    tmp = torch.load(files_path)\n",
    "    tmp_npy = tmp.numpy()\n",
    "    np.save(os.path.join(dest_path,file.replace('.pt','.npy')),tmp_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latent code by optimization\n",
    "\n",
    "import os\n",
    "path_test_imgs = '/media/pc/LabServers/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/'\n",
    "dirs = ['epoch_89','epoch_93','epoch_94','epoch_100', 'epoch_106', 'epoch_112']\n",
    "for di in dirs:\n",
    "    \n",
    "    files = os.listdir(os.path.join(path_test_imgs,di))\n",
    "    print(len(files))\n",
    "    for file in files:\n",
    "        if '.png' in file:\n",
    "            path_file = os.path.join(path_test_imgs,di,file)\n",
    "            # cmd = f'CUDA_VISIBLE_DEVICES=0 python ./run_projector.py --image_path {path_file} --datajson /media/pc/LabServers/hengda/datasets/MT-Dataset/images/mirror_non_makeup/all_mt_dataset.json --latent_space_type w_plus --outdir=../results/optimize_out --network ../pretrained_models/ffhq512-128.pkl'\n",
    "            # os.system(cmd)       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyq_eg3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd8aa1e353085ab8c9d6acf1eecd95a62bbd8c85e2ba57ed7c25982a5575dde6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
