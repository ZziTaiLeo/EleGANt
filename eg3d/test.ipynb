{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify datajson\n",
    "import torch\n",
    "import os \n",
    "import json \n",
    "datajson_path_makeup = '/media/pc/hengda1t/hengda/datasets/MT-Dataset/images/mirror_makeup/mirror_makeup_dataset.json'\n",
    "datajson_path_non_makeup = '/media/pc/hengda1t/hengda/datasets/MT-Dataset/images/mirror_non_makeup/mirror_non_makeup_dataset.json'\n",
    "\n",
    "with open(datajson_path_non_makeup,'r') as f:\n",
    "    datajson_non_makeup = json.load(f)\n",
    "with open(datajson_path_makeup,'r') as f:\n",
    "    datajson_makeup = json.load(f)\n",
    "\n",
    "print(len(datajson_non_makeup['labels']))\n",
    "for x in datajson_makeup['labels']:\n",
    "    datajson_non_makeup['labels'].append(x)\n",
    "with open ('/media/pc/hengda1t/hengda/datasets/MT-Dataset/images/mirror_non_makeup/all_mt_dataset.json','w') as f:\n",
    "   json.dump(datajson_non_makeup,f)\n",
    "print(len(datajson_non_makeup['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_file is : vRX522.png\n",
      "error_file is : vRX429.png\n",
      "error_file is : vRX430_mirror.png\n",
      "error_file is : vRX376_mirror.png\n",
      "error_file is : vRX336.png\n",
      "error_file is : vHX457.png\n",
      "error_file is : vHX457_mirror.png\n",
      "error_file is : fd4f8721154ec707edd399272e67b9ba.png\n"
     ]
    }
   ],
   "source": [
    "# landmarks\n",
    "import dlib\n",
    "import numpy as np\n",
    "def get_landmark(filepath, predictor):\n",
    "    \"\"\"get landmark with dlib\n",
    "    :return: np.array shape=(68, 2)\n",
    "    \"\"\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    img = dlib.load_rgb_image(filepath)\n",
    "    dets = detector(img, 1)\n",
    "    for k, d in enumerate(dets):\n",
    "        shape = predictor(img, d)\n",
    "\n",
    "    t = list(shape.parts())\n",
    "    a = []\n",
    "    for tt in t:\n",
    "        a.append([tt.x, tt.y])\n",
    "    lm = np.array(a)\n",
    "    return lm\n",
    "model_path = '../pretrained_models/shape_predictor_68_face_landmarks.dat'\n",
    "predictor = dlib.shape_predictor(\n",
    "    model_path)\n",
    "\n",
    "input_dir_type = 'makeup'\n",
    "input_img_path = f'/media/pc/hengda1t/hengda/datasets/MT-Dataset/images/{input_dir_type}'\n",
    "files = os.listdir(input_img_path)\n",
    "save_dir_path = f'/media/pc/hengda1t/hengda/datasets/MT-Dataset/lms_crop/{input_dir_type}'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "for file in files:\n",
    "    if '.png' in file:\n",
    "        try:\n",
    "            source_img_path = os.path.join(input_img_path,file)\n",
    "            lm = get_landmark(source_img_path, predictor)\n",
    "            np.save(os.path.join(save_dir_path,file.replace('.png','.npy')), lm)\n",
    "        except:\n",
    "            print('error_file is :',file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmarks\n",
    "import dlib\n",
    "import numpy as np\n",
    "def get_landmark(filepath, predictor):\n",
    "    \"\"\"get landmark with dlib\n",
    "    :return: np.array shape=(68, 2)\n",
    "    \"\"\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    img = dlib.load_rgb_image(filepath)\n",
    "    dets = detector(img, 1)\n",
    "    for k, d in enumerate(dets):\n",
    "        shape = predictor(img, d)\n",
    "\n",
    "    t = list(shape.parts())\n",
    "    a = []\n",
    "    for tt in t:\n",
    "        a.append([tt.x, tt.y])\n",
    "    lm = np.array(a)\n",
    "    return lm\n",
    "model_path = '../pretrained_models/shape_predictor_68_face_landmarks.dat'\n",
    "predictor = dlib.shape_predictor(\n",
    "    model_path)\n",
    "\n",
    "input_dir_type = 'makeup'\n",
    "input_img_path = f'/media/pc/hengda1t/hengda/datasets/MT-Dataset/images/{input_dir_type}'\n",
    "files = os.listdir(input_img_path)\n",
    "save_dir_path = f'/media/pc/hengda1t/hengda/datasets/MT-Dataset/lms_crop/{input_dir_type}'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "for file in files:\n",
    "    if 'png' in file:\n",
    "        source_img_path = os.path.join(input_img_path,file)\n",
    "        lm = get_landmark(source_img_path, predictor)\n",
    "        np.save(os.path.join(save_dir_path,file.replace('.png','.npy')), lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# preprocess mask\n",
    "import os\n",
    "import torch\n",
    "import numpy as np \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional\n",
    "from PIL import Image\n",
    "path_mask = '/media/pc/hengda1t/hengda/EleGANt-eg3d/EleGANt/data_for_test/mask/11220_parsing.png'\n",
    "lip_class = [12,13]\n",
    "eyebrow_class =[2,3]\n",
    "face_class = [1,10]\n",
    "eye_class = [4,5]\n",
    "def load_mask(path):\n",
    "    mask = np.array(Image.open(path).convert('L'))\n",
    "    mask = torch.FloatTensor(mask).unsqueeze(0)\n",
    "    mask = functional.resize(mask, 512, transforms.InterpolationMode.NEAREST)\n",
    "    return mask\n",
    "def mask_process( mask: torch.tensor):\n",
    "    '''\n",
    "    mask: (1, h, w)\n",
    "    '''        \n",
    "    mask_lip = (mask == lip_class[0]).float() + (mask == lip_class[1]).float()\n",
    "    mask_face = (mask == face_class[0]).float() + (mask == face_class[1]).float()\n",
    "\n",
    "    #mask_eyebrow_left = (mask == eyebrow_class[0]).float()\n",
    "    #mask_eyebrow_right = (mask == eyebrow_class[1]).float()\n",
    "    mask_face += (mask == eyebrow_class[0]).float()\n",
    "    mask_face += (mask == eyebrow_class[1]).float()\n",
    "\n",
    "    mask_eye_left = (mask == eye_class[0]).float()\n",
    "    mask_eye_right = (mask == eye_class[1]).float()\n",
    "\n",
    "    #mask_list = [mask_lip, mask_face, mask_eyebrow_left, mask_eyebrow_right, mask_eye_left, mask_eye_right]\n",
    "    mask_list = [mask_lip, mask_face, mask_eye_left, mask_eye_right]\n",
    "    mask_aug = torch.cat(mask_list, 0) # (c, h, w)\n",
    "    return mask_aug      \n",
    "\n",
    "mask = load_mask(path_mask)\n",
    "out = mask_process(mask)\n",
    "print(mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder result latents  .pt-> .npy\n",
    "import torch \n",
    "import numpy as np \n",
    "path = '/media/pc/hengda1t/hengda/e4e_eg3d/save_encoder/inversions/latents/'\n",
    "dest_path = '/media/pc/hengda1t/hengda/EG3D-projector/eg3d/projector_out/encoder'\n",
    "files = os.listdir(path)\n",
    "for file in files:\n",
    "    files_path = os.path.join(path,file)\n",
    "    tmp = torch.load(files_path)\n",
    "    tmp_npy = tmp.numpy()\n",
    "    np.save(os.path.join(dest_path,file.replace('.pt','.npy')),tmp_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "root = '/media/pc/hengda1t/hengda/datasets/test_cele'\n",
    "dataset_json = os.path.join(root,'dataset.json')\n",
    "root_pngs = os.listdir(root)\n",
    "with open(dataset_json,'r') as f :\n",
    "    dataset = json.load(f)\n",
    "    dataset_labels = dict(dataset['labels'])\n",
    "\n",
    "for file in root_pngs:\n",
    "    if '.png' in file:\n",
    "        camera = dataset_labels[file]      \n",
    "        np.save(os.path.join(root,file.replace('.png','.npy')),np.array(camera,np.float32))\n",
    "\n",
    "\n",
    "    print('is over')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latent code by optimization\n",
    "\n",
    "import os\n",
    "path_test_imgs = '/media/pc/hengda1t/hengda/EleGANt-eg3d/EleGANt/data_for_test/test_mt_optimize/'\n",
    "dirs = ['epoch_89','epoch_93','epoch_94','epoch_100', 'epoch_106', 'epoch_112']\n",
    "for di in dirs:\n",
    "    \n",
    "    files = os.listdir(os.path.join(path_test_imgs,di))\n",
    "    print(len(files))\n",
    "    for file in files:\n",
    "        if '.png' in file:\n",
    "            path_file = os.path.join(path_test_imgs,di,file)\n",
    "            # cmd = f'CUDA_VISIBLE_DEVICES=0 python ./run_projector.py --image_path {path_file} --datajson /media/pc/hengda1t/hengda/datasets/MT-Dataset/images/mirror_non_makeup/all_mt_dataset.json --latent_space_type w_plus --outdir=../results/optimize_out --network ../pretrained_models/ffhq512-128.pkl'\n",
    "            # os.system(cmd)       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wyq_eg3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd8aa1e353085ab8c9d6acf1eecd95a62bbd8c85e2ba57ed7c25982a5575dde6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
